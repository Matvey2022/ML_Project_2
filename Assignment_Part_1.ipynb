{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment (First Part)\n",
    "##### Name: Matvey Makhnov<br> \n",
    "Task 1: Detection of inconsistencies in flower descriptions in online floristry and delivery platforms is essential for success, customer retention, and satisfaction. Many companies providing online floristry services are increasingly utilizing deep learning solutions to ensure that a flower image displayed on their platform matches the given description or category. <br> <br>To implement a flower classification convolutional neural network (CNN) trained on the Flowers102 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# set a size for our batches \n",
    "train_batch = 32\n",
    "val_batch = 32 \n",
    "test_batch = 32\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "validation_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225])\n",
    "]) \n",
    "\n",
    "\n",
    "data_path = os.path.join('.', 'Flowers_102_Dataset')\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#dataset = datasets.Flowers102(root = data_path, transform=None, download=True)\n",
    "# set a size for our sets (training, validation, test) \n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#val_size = int(0.1 * len(dataset))\n",
    "#test_size = len(dataset) - (train_size + val_size)\n",
    "#train_dataset, val_dataset, test_dataset =  random_split(dataset, [train_size,val_size,test_size])\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# I want to apply 2 different transforms compositions \n",
    "# that is why I'll use next code \n",
    "train_dataset = datasets.Flowers102(root = data_path, split=\"train\", transform=train_transform, download=True)\n",
    "val_dataset = datasets.Flowers102(root = data_path, split=\"val\", transform=validation_transform, download=True) \n",
    "test_dataset = datasets.Flowers102(root = data_path, split=\"test\", transform=validation_transform, download=True)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=train_batch,shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=val_batch,shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=test_batch, shuffle=False)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build our CNN architecture depends on Table 1 in  `F24.ML.Assignment.2.pdf` file <br> In this architecture I'll use only RELU activation function for all layes and for last one I'll apply softmax to get final results. In total we'll have 102 classes cause we have 102 types of flowers in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "class CNN_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_1,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1,padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=128 * 28 * 28, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512,out_features=102)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        # the 1st convol layer input 224x224x3 output 114x112x32\n",
    "        x = self.pool(self.conv1(x))\n",
    "        # # the 2nd convol layer input 112x112x32 output 56x56x64\n",
    "        x = self.pool(self.conv2(x))\n",
    "        # the 3rd convol layer input 56x56x64 output 28x28x128\n",
    "        x = self.pool(self.conv3(x))\n",
    "\n",
    "        # here we have 28*28*128 values of feature map \n",
    "\n",
    "        # flattening \n",
    "        x = x.view(-1, 128 * 28 * 28)   \n",
    "\n",
    "\n",
    "        # using weight matrics to \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x=self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim = 1)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_1 = CNN_1().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in our 1st CNN model: 51526310\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters in our 1st CNN model: {counter_params(model_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020\n",
      "1020\n",
      "6149\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll build training, validation and test function <br> We have to estimate our model on training, validation and test sets with using accuracy, loss and F1-score. I'll calculate average loss and accuracy for training set. And for validation and test sets I'll apply all of them (accuracy, average loss and definitely F1-score, cause it will give us ability to understand how to make our model better). <br> As a loss function I'll choose NLL Loss (Will explain latter why I choose it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_1(model, device, loader, dataset, optimizer, epoch, writer):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    train_correct = 0 \n",
    "    total = 0 \n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(loader, desc=f\"Training epoch: {epoch}\")):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = F.nll_loss(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss +=loss.item()\n",
    "        prediction = output.argmax(dim = 1, keepdim = True)\n",
    "        #train_correct += (prediction == labels).sum().item()\n",
    "        train_correct += prediction.eq(labels.view_as(prediction)).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    average_loss = train_loss/len(loader)\n",
    "    accuracy = train_correct/len(loader.dataset) * 100.0 \n",
    "\n",
    "    writer.add_scalar('Loss/Train', average_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Train', accuracy, epoch)\n",
    "    writer.flush()\n",
    "    \n",
    "\n",
    "    print(f\"==> Epoch {epoch} Completed: Average loss: {average_loss:.6f}\\tAccuracy: {accuracy:.3f}% \")\n",
    "\n",
    "\n",
    "def validation_1(model, device, loader, dataset, epoch, writer):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    val_correct = 0 \n",
    "    v_labels_list = []\n",
    "    v_prediction_list = []\n",
    "    total = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Valodation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            validation_loss +=loss.item()\n",
    "\n",
    "            prediction = output.argmax(dim = 1, keepdim = True)\n",
    "            #val_correct += (prediction == labels).sum().item()\n",
    "            val_correct += prediction.eq(labels.view_as(prediction)).sum().item()\n",
    "\n",
    "            v_labels_list.extend(labels.cpu().numpy())\n",
    "            v_prediction_list.extend(prediction.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            \n",
    "\n",
    "    average_loss = validation_loss/len(loader)\n",
    "    accuracy = val_correct/len(loader.dataset) * 100.0\n",
    "    f1 = f1_score(v_labels_list, v_prediction_list, average=\"weighted\")\n",
    "\n",
    "    writer.add_scalar('Loss/Validation', average_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Validation', accuracy, epoch)\n",
    "    writer.add_scalar('F1-score/Validation', f1, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    print(f\"==> Validation Completed: Average Loss: {average_loss:.6f}\\tAccuracy: {accuracy:.2f}%\\tF-1 Score: {f1:.4f}\")\n",
    "\n",
    "    # return accuracy and loss for tracking \n",
    "    return average_loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "def test_1(model, device, loader,dataset):\n",
    "    model.eval()\n",
    "    test_loss = 0 \n",
    "    test_correct = 0\n",
    "    t_label_list = []\n",
    "    t_prediction_list = []\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Test\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            test_loss +=loss.item()\n",
    "\n",
    "            prediction = output.argmax(dim = 1, keepdim = True)\n",
    "            #test_correct += (prediction == labels).sum().item()\n",
    "            test_correct += prediction.eq(labels.view_as(prediction)).sum().item()\n",
    "\n",
    "            t_label_list.extend(labels.cpu().numpy())\n",
    "            t_prediction_list.extend(prediction.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    average_loss = test_loss/len(loader)\n",
    "    accuracy = test_correct/len(loader.dataset) * 100\n",
    "    f1 = f1_score(t_label_list,t_prediction_list,average=\"weighted\")\n",
    "\n",
    "    print(f\"==>Test Completed: Avverage loss: {average_loss:.6f}\\tAccuracy: {accuracy:.2f}%\\tF-1 Score: {f1:.4f}\")\n",
    "\n",
    "    # return for tracking \n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll train my model with applying SGD, learning rate = 0.001. Also I'll use TensorBoard <br> TensorBorad will help us to: <br>1. Inspect the model architecture <br>2. Create interactive of the visualization (...) <br> [Link for TensorBoard documentation](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 1:   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 1: 100%|██████████| 32/32 [01:04<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 1 Completed: Average loss: 4.056027\tAccuracy: 11.569% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:25<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 4.087844\tAccuracy: 10.49%\tF-1 Score: 0.0573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 193/193 [02:59<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Test Completed: Avverage loss: 4.143671\tAccuracy: 7.55%\tF-1 Score: 0.0399\n",
      "The best model was saved with accuracy: 7.55%\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 2: 100%|██████████| 32/32 [01:05<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 2 Completed: Average loss: 3.846391\tAccuracy: 15.294% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:26<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.943470\tAccuracy: 12.55%\tF-1 Score: 0.0806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 193/193 [02:42<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Test Completed: Avverage loss: 4.016772\tAccuracy: 9.63%\tF-1 Score: 0.0704\n",
      "The best model was saved with accuracy: 9.63%\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 3: 100%|██████████| 32/32 [01:04<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 3 Completed: Average loss: 3.614059\tAccuracy: 19.804% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:23<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.809989\tAccuracy: 12.25%\tF-1 Score: 0.0827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 193/193 [02:52<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Test Completed: Avverage loss: 3.897364\tAccuracy: 10.60%\tF-1 Score: 0.0781\n",
      "The best model was saved with accuracy: 10.60%\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 4: 100%|██████████| 32/32 [01:03<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 4 Completed: Average loss: 3.374813\tAccuracy: 23.333% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:23<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.713483\tAccuracy: 13.63%\tF-1 Score: 0.1087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 193/193 [02:51<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Test Completed: Avverage loss: 3.831574\tAccuracy: 10.83%\tF-1 Score: 0.0870\n",
      "The best model was saved with accuracy: 10.83%\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 5: 100%|██████████| 32/32 [01:06<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 5 Completed: Average loss: 3.140144\tAccuracy: 25.686% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:31<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.624427\tAccuracy: 15.39%\tF-1 Score: 0.1299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 193/193 [03:02<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Test Completed: Avverage loss: 3.776825\tAccuracy: 13.74%\tF-1 Score: 0.1198\n",
      "The best model was saved with accuracy: 13.74%\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 6: 100%|██████████| 32/32 [01:15<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 6 Completed: Average loss: 2.892357\tAccuracy: 31.765% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:34<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.596685\tAccuracy: 15.10%\tF-1 Score: 0.1258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 193/193 [03:22<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Test Completed: Avverage loss: 3.768513\tAccuracy: 12.20%\tF-1 Score: 0.1085\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 7: 100%|██████████| 32/32 [01:27<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 7 Completed: Average loss: 2.688058\tAccuracy: 34.608% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.540373\tAccuracy: 17.84%\tF-1 Score: 0.1529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 193/193 [02:25<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Test Completed: Avverage loss: 3.672155\tAccuracy: 14.41%\tF-1 Score: 0.1298\n",
      "The best model was saved with accuracy: 14.41%\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 8: 100%|██████████| 32/32 [00:59<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 8 Completed: Average loss: 2.480470\tAccuracy: 40.000% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:24<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.598546\tAccuracy: 16.67%\tF-1 Score: 0.1385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 193/193 [03:26<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Test Completed: Avverage loss: 3.761716\tAccuracy: 14.67%\tF-1 Score: 0.1264\n",
      "The best model was saved with accuracy: 14.67%\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 9: 100%|██████████| 32/32 [01:36<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 9 Completed: Average loss: 2.372143\tAccuracy: 40.490% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.515098\tAccuracy: 18.04%\tF-1 Score: 0.1641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 193/193 [03:39<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Test Completed: Avverage loss: 3.737706\tAccuracy: 15.06%\tF-1 Score: 0.1417\n",
      "The best model was saved with accuracy: 15.06%\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 10: 100%|██████████| 32/32 [01:03<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 10 Completed: Average loss: 2.157302\tAccuracy: 44.804% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:24<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.577697\tAccuracy: 19.02%\tF-1 Score: 0.1626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 193/193 [02:30<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Test Completed: Avverage loss: 3.830039\tAccuracy: 14.82%\tF-1 Score: 0.1293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    " \n",
    "# before starting our training we have to \n",
    "# Initialize tensorboard writer\n",
    "writer = SummaryWriter(log_dir=\"First_CNN_Model\")\n",
    "\n",
    "# our hyperparameters \n",
    "epochs = 10 \n",
    "learning_rate = 0.001\n",
    "momentum = 0.5\n",
    "\n",
    "# Model and optimizer \n",
    "model = model_1.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "best_accuracy = 0 \n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "    \n",
    "    # start our training\n",
    "    train_1(model, device, train_loader,train_dataset, optimizer, epoch, writer)\n",
    "\n",
    "    validation_1(model, device, val_loader, val_dataset, epoch, writer)\n",
    "\n",
    "    test_loss, test_accuracy = test_1(model, device, test_loader, test_dataset)\n",
    "\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), \"Best_in_the_1st_CNN.pt\")\n",
    "        print(f\"The best model was saved with accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# in the end we have to use close method \n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to solve next problems: <br> 1. Figure out with accuracy why it's more than 100% <br> 2. Cnahge place where I'll put some writer cause in the end i just got point on my plot  (just wait untill pogram will not finished full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will create the 2nd model (optimize my 1st model with using next technic):<br> 1. Batch normalization <br> 2. Early stopping <br> 3. Dropout <br> 4. Scheduler for learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
