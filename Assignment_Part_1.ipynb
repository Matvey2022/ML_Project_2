{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment (First Part)\n",
    "##### Name: Matvey Makhnov<br> \n",
    "Task 1: Detection of inconsistencies in flower descriptions in online floristry and delivery platforms is essential for success, customer retention, and satisfaction. Many companies providing online floristry services are increasingly utilizing deep learning solutions to ensure that a flower image displayed on their platform matches the given description or category. <br> <br>To implement a flower classification convolutional neural network (CNN) trained on the Flowers102 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# set a size for our batches \n",
    "train_batch = 32\n",
    "val_batch = 32 \n",
    "test_batch = 32\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "validation_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225])\n",
    "]) \n",
    "\n",
    "\n",
    "data_path = os.path.join('.', 'Flowers_102_Dataset')\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#dataset = datasets.Flowers102(root = data_path, transform=None, download=True)\n",
    "# set a size for our sets (training, validation, test) \n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#val_size = int(0.1 * len(dataset))\n",
    "#test_size = len(dataset) - (train_size + val_size)\n",
    "#train_dataset, val_dataset, test_dataset =  random_split(dataset, [train_size,val_size,test_size])\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# I want to apply 2 different transforms compositions \n",
    "# that is why I'll use next code \n",
    "train_dataset = datasets.Flowers102(root = data_path, split=\"train\", transform=train_transform, download=True)\n",
    "val_dataset = datasets.Flowers102(root = data_path, split=\"val\", transform=validation_transform, download=True) \n",
    "test_dataset = datasets.Flowers102(root = data_path, split=\"test\", transform=validation_transform, download=True)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=train_batch,shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=val_batch,shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=test_batch, shuffle=False)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build our CNN architecture depends on Table 1 in  `F24.ML.Assignment.2.pdf` file <br> In this architecture I'll use only RELU activation function for all layes and for last one I'll apply softmax to get final results. In total we'll have 102 classes cause we have 102 types of flowers in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "class CNN_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_1,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1,padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=128 * 28 * 28, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512,out_features=102)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        # the 1st convol layer input 224x224x3 output 114x112x32\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # # the 2nd convol layer input 112x112x32 output 56x56x64\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # the 3rd convol layer input 56x56x64 output 28x28x128\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        # here we have 28*28*128 values of feature map \n",
    "\n",
    "        # flattening \n",
    "        x = x.view(-1, 128 * 28 * 28)   \n",
    "\n",
    "\n",
    "        # using weight matrics to \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x=self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim = 1)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_1 = CNN_1().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in our 1st CNN model: 51526310\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters in our 1st CNN model: {counter_params(model_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020\n",
      "1020\n",
      "6149\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll build training, validation and test function <br> We have to estimate our model on training, validation and test sets with using accuracy, loss and F1-score. I'll calculate average loss and accuracy for training set. And for validation and test sets I'll apply all of them (accuracy, average loss and definitely F1-score, cause it will give us ability to understand how to make our model better). <br> As a loss function I'll choose NLL Loss (Will explain latter why I choose it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_1(model, device, loader, dataset, optimizer, epoch, writer):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    train_correct = 0 \n",
    "    total = 0 \n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(loader, desc=f\"Training epoch: {epoch}\")):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = F.nll_loss(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss +=loss.item()\n",
    "        prediction = output.argmax(dim = 1, keepdim = True)\n",
    "        #train_correct += (prediction == labels).sum().item()\n",
    "        train_correct += prediction.eq(labels.view_as(prediction)).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    average_loss = train_loss/len(loader)\n",
    "    accuracy = train_correct/len(loader.dataset) * 100.0 \n",
    "\n",
    "    writer.add_scalar('Loss/Train', average_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Train', accuracy, epoch)\n",
    "    writer.flush()\n",
    "    \n",
    "\n",
    "    print(f\"==> Epoch {epoch} Completed: Average loss: {average_loss:.6f}\\tAccuracy: {accuracy:.3f}% \")\n",
    "\n",
    "\n",
    "def validation_1(model, device, loader, dataset, epoch, writer):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    val_correct = 0 \n",
    "    v_labels_list = []\n",
    "    v_prediction_list = []\n",
    "    total = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Valodation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            validation_loss +=loss.item()\n",
    "\n",
    "            prediction = output.argmax(dim = 1, keepdim = True)\n",
    "            #val_correct += (prediction == labels).sum().item()\n",
    "            val_correct += prediction.eq(labels.view_as(prediction)).sum().item()\n",
    "\n",
    "            v_labels_list.extend(labels.cpu().numpy())\n",
    "            v_prediction_list.extend(prediction.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            \n",
    "\n",
    "    average_loss = validation_loss/len(loader)\n",
    "    accuracy = val_correct/len(loader.dataset) * 100.0\n",
    "    f1 = f1_score(v_labels_list, v_prediction_list, average=\"weighted\")\n",
    "\n",
    "    writer.add_scalar('Loss/Validation', average_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Validation', accuracy, epoch)\n",
    "    writer.add_scalar('F1-score/Validation', f1, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    print(f\"==> Validation Completed: Average Loss: {average_loss:.6f}\\tAccuracy: {accuracy:.2f}%\\tF-1 Score: {f1:.4f}\")\n",
    "\n",
    "    # return accuracy and loss for tracking \n",
    "    return average_loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "def test_1(model, device, loader,dataset):\n",
    "    model.eval()\n",
    "    test_loss = 0 \n",
    "    test_correct = 0\n",
    "    t_label_list = []\n",
    "    t_prediction_list = []\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Test\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            test_loss +=loss.item()\n",
    "\n",
    "            prediction = output.argmax(dim = 1, keepdim = True)\n",
    "            #test_correct += (prediction == labels).sum().item()\n",
    "            test_correct += prediction.eq(labels.view_as(prediction)).sum().item()\n",
    "\n",
    "            t_label_list.extend(labels.cpu().numpy())\n",
    "            t_prediction_list.extend(prediction.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    average_loss = test_loss/len(loader)\n",
    "    accuracy = test_correct/len(loader.dataset) * 100\n",
    "    f1 = f1_score(t_label_list,t_prediction_list,average=\"weighted\")\n",
    "\n",
    "    print(f\"==>Test Completed: Avverage loss: {average_loss:.6f}\\tAccuracy: {accuracy:.2f}%\\tF-1 Score: {f1:.4f}\")\n",
    "\n",
    "    # return for tracking \n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll train my model with applying SGD, learning rate = 0.001. Also I'll use TensorBoard <br> TensorBorad will help us to visualize our Average Loss, Accuracy and F1-score during whole training process<br> [Link for TensorBoard documentation](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 1:   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 1: 100%|██████████| 32/32 [01:33<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 1 Completed: Average loss: 4.616038\tAccuracy: 1.569% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:35<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 4.582602\tAccuracy: 4.12%\tF-1 Score: 0.0156\n",
      "The best model was saved with accuracy: 4.12%\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 2: 100%|██████████| 32/32 [01:13<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 2 Completed: Average loss: 4.554125\tAccuracy: 5.196% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:31<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 4.530618\tAccuracy: 5.78%\tF-1 Score: 0.0257\n",
      "The best model was saved with accuracy: 5.78%\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 3: 100%|██████████| 32/32 [01:09<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 3 Completed: Average loss: 4.481274\tAccuracy: 6.765% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:30<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 4.462608\tAccuracy: 5.98%\tF-1 Score: 0.0267\n",
      "The best model was saved with accuracy: 5.98%\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 4: 100%|██████████| 32/32 [01:09<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 4 Completed: Average loss: 4.383176\tAccuracy: 7.157% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:27<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 4.372005\tAccuracy: 6.37%\tF-1 Score: 0.0315\n",
      "The best model was saved with accuracy: 6.37%\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 5: 100%|██████████| 32/32 [01:08<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 5 Completed: Average loss: 4.250586\tAccuracy: 8.725% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:27<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 4.259804\tAccuracy: 7.35%\tF-1 Score: 0.0403\n",
      "The best model was saved with accuracy: 7.35%\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 6: 100%|██████████| 32/32 [01:11<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 6 Completed: Average loss: 4.090029\tAccuracy: 11.373% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:31<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 4.127021\tAccuracy: 9.31%\tF-1 Score: 0.0532\n",
      "The best model was saved with accuracy: 9.31%\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 7: 100%|██████████| 32/32 [01:10<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 7 Completed: Average loss: 3.885299\tAccuracy: 13.627% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:31<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.997244\tAccuracy: 10.98%\tF-1 Score: 0.0706\n",
      "The best model was saved with accuracy: 10.98%\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 8: 100%|██████████| 32/32 [01:11<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 8 Completed: Average loss: 3.668203\tAccuracy: 18.627% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:32<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.883033\tAccuracy: 10.20%\tF-1 Score: 0.0682\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 9: 100%|██████████| 32/32 [01:12<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 9 Completed: Average loss: 3.435644\tAccuracy: 20.098% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:34<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.767782\tAccuracy: 13.63%\tF-1 Score: 0.0919\n",
      "The best model was saved with accuracy: 13.63%\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 10: 100%|██████████| 32/32 [01:10<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 10 Completed: Average loss: 3.189563\tAccuracy: 25.588% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:25<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.690553\tAccuracy: 14.51%\tF-1 Score: 0.1132\n",
      "The best model was saved with accuracy: 14.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 193/193 [03:01<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Test Completed: Avverage loss: 3.807991\tAccuracy: 13.79%\tF-1 Score: 0.1087\n",
      "The best result was saved with Accuracy: 13.79% and Average Loss: 3.807991\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    " \n",
    "# before starting our training we have to \n",
    "# Initialize tensorboard writer\n",
    "writer = SummaryWriter(log_dir=\"First_CNN_Model\")\n",
    "\n",
    "# our hyperparameters \n",
    "epochs = 10 \n",
    "learning_rate = 0.001\n",
    "momentum = 0.5\n",
    "\n",
    "# Model and optimizer \n",
    "model = model_1.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "best_accuracy = 0 \n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "    \n",
    "    # start our training\n",
    "    train_1(model, device, train_loader,train_dataset, optimizer, epoch, writer)\n",
    "\n",
    "    val_loss, val_accuracy = validation_1(model, device, val_loader, val_dataset, epoch, writer)\n",
    "\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), \"Best_in_the_1st_CNN.pt\")\n",
    "        print(f\"The best model was saved with accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "# test our model on test set\n",
    "test_loss, test_accuracy = test_1(model, device, test_loader, test_dataset)\n",
    "torch.save(model.state_dict(), \"Test_result_for_1st_CNN.pt\")\n",
    "print(f\"The best result was saved with Accuracy: {test_accuracy:.2f}% and Average Loss: {test_loss:.6f}\")\n",
    "\n",
    "# in the end we have to use close method \n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "We can see that value of final accuracy on test set is too low - ` 13.9% `. It says to us that we have to optimize our model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now I will create the 2nd model (optimize my 1st model with using next technic):**<br> 1. Batch normalization (nn.BatchNorm2d for convolution Layers and nn.Batchnormalization1d for FC Layers ) <br> 2. Early stopping <br> 3. Dropout (p = 0.5 for FC layer) <br> 4. Scheduler for learning rate (Will emplement in the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "class CNN_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_2, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32,kernel_size=3, stride=1, padding=1)\n",
    "        self.batch1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1,padding=1)\n",
    "        self.batch3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #self.conv_dropout = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=128 * 28 * 28, out_features=512)\n",
    "        self.fc_batch1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(in_features=512,out_features=102)\n",
    "\n",
    "        self.fc_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.pool(F.relu(self.batch1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.batch2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.batch3(self.conv3(x))))\n",
    "\n",
    "        x = x.view(-1,128*28*28)\n",
    "\n",
    "        x = self.fc_dropout(F.relu(self.fc_batch1(self.fc1(x))))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_2 = CNN_2().to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in our 1st CNN model: 51527782\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters in our 1st CNN model: {counter_params(model_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_2(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=100352, out_features=512, bias=True)\n",
      "  (fc_batch1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=102, bias=True)\n",
      "  (fc_dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# just to see our archetecture \n",
    "print(model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use previous funcrion: ` train_1 `, ` validation_1 `, ` test_1 `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 1: 100%|██████████| 32/32 [01:23<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 1 Completed: Average loss: 4.369323\tAccuracy: 5.588% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:29<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 4.037020\tAccuracy: 13.14%\tF-1 Score: 0.0860\n",
      "The best model was saved with accuracy: 13.14%\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 2: 100%|██████████| 32/32 [01:19<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 2 Completed: Average loss: 3.605604\tAccuracy: 21.471% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:30<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.607573\tAccuracy: 20.98%\tF-1 Score: 0.1576\n",
      "The best model was saved with accuracy: 20.98%\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 3: 100%|██████████| 32/32 [01:19<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 3 Completed: Average loss: 3.152220\tAccuracy: 34.118% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:28<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.344013\tAccuracy: 27.75%\tF-1 Score: 0.2262\n",
      "The best model was saved with accuracy: 27.75%\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 4: 100%|██████████| 32/32 [01:19<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 4 Completed: Average loss: 2.800133\tAccuracy: 43.333% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:29<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.228951\tAccuracy: 31.57%\tF-1 Score: 0.2702\n",
      "The best model was saved with accuracy: 31.57%\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 5: 100%|██████████| 32/32 [01:22<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 5 Completed: Average loss: 2.454779\tAccuracy: 55.784% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:32<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.063599\tAccuracy: 34.41%\tF-1 Score: 0.2998\n",
      "The best model was saved with accuracy: 34.41%\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 6: 100%|██████████| 32/32 [01:18<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 6 Completed: Average loss: 2.219969\tAccuracy: 61.471% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:28<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 2.934191\tAccuracy: 35.59%\tF-1 Score: 0.3165\n",
      "The best model was saved with accuracy: 35.59%\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 7: 100%|██████████| 32/32 [01:19<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 7 Completed: Average loss: 1.937748\tAccuracy: 70.294% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:29<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 2.887398\tAccuracy: 35.88%\tF-1 Score: 0.3186\n",
      "The best model was saved with accuracy: 35.88%\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 8: 100%|██████████| 32/32 [01:18<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 8 Completed: Average loss: 1.720086\tAccuracy: 76.373% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:28<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 2.791682\tAccuracy: 38.53%\tF-1 Score: 0.3520\n",
      "The best model was saved with accuracy: 38.53%\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 9: 100%|██████████| 32/32 [01:18<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 9 Completed: Average loss: 1.506288\tAccuracy: 80.980% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:28<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 2.776209\tAccuracy: 38.14%\tF-1 Score: 0.3496\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 10: 100%|██████████| 32/32 [01:17<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 10 Completed: Average loss: 1.296220\tAccuracy: 85.686% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [00:29<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 2.687117\tAccuracy: 39.61%\tF-1 Score: 0.3722\n",
      "The best model was saved with accuracy: 39.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 193/193 [02:58<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Test Completed: Avverage loss: 2.874494\tAccuracy: 34.64%\tF-1 Score: 0.3367\n",
      "Test model was saved with Accuracy: 34.64% and Avverage Loss: 2.874494\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "writer_2 = SummaryWriter(log_dir=\"Second_CNN_Model\")\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "# For early stopping\n",
    "patience = 4 \n",
    "\n",
    "model = model_2.to(device)\n",
    "optimizer_2 = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "scheduler = StepLR(optimizer_2, step_size=5, gamma=0.1)\n",
    "\n",
    "best_accuracy_2 = 0 \n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "    train_1(model,device, train_loader,train_dataset,optimizer_2, epoch, writer_2)\n",
    "\n",
    "    val_loss, val_acc = validation_1(model, device, val_loader, val_dataset, epoch, writer_2)\n",
    "\n",
    "    # control condition for early stopping \n",
    "    if val_acc>best_accuracy_2:\n",
    "        best_accuracy_2 = val_acc\n",
    "        torch.save(model.state_dict(), \"Best_in_the_2nd_CNN.pt\")\n",
    "        print(f\"The best model was saved with accuracy: {best_accuracy_2:.2f}%\")\n",
    "        early_stop_counter = 0\n",
    "    else: \n",
    "        early_stop_counter +=1 \n",
    "\n",
    "    # early stopping codition\n",
    "    if early_stop_counter>=patience:\n",
    "        print(f\"Early stopping at epoch {epoch} due to no improvement in test accuracy.\")\n",
    "        break\n",
    "    \n",
    "    # step the schedular\n",
    "    scheduler.step()\n",
    "\n",
    "test_loss, test_acc = test_1(model, device, test_loader, test_dataset)\n",
    "\n",
    "torch.save(model.state_dict(), \"Test_result_for_2nd_CNN.pt\")\n",
    "print(f'Test model was saved with Accuracy: {test_acc:.2f}% and Avverage Loss: {test_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "We can see how our perfomance became batter thanks to optimization. Before optimization Accuracy on test data was ` 13.79% ` and after optimization we got ` 34.64% `. <br>It's influence of Batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Task:** Use transfer learning to achieve better performance than the improved baseline model above. I'll use pretrained model `resnet50` <br> I've learned how to implement transfer learning over [here](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\user/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:03<00:00, 30.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Let's download our pretrained model \n",
    "import torch \n",
    "from torchvision import models\n",
    "\n",
    "model_3 = models.resnet50(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in our Pretrained resnet50 model: 25557032\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters in our Pretrained resnet50 model: {counter_params(model_3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# just to see our architecture of FC Layers \n",
    "print(model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I'm freezing the model's pretrained weights\n",
    "for params in model_3.parameters():\n",
    "    params.requires_grad = False \n",
    "\n",
    "# reset final fully connected layer\n",
    "#fc_input = model_3.fc.in_features\n",
    "\n",
    "model_3.fc = nn.Sequential(\n",
    "    nn.Linear(2048,102),\n",
    "    nn.LogSoftmax(dim=1) # cause we are using NLL Loss function in our Loss and we need to apply LogSoftmax to get correct values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2048, out_features=102, bias=True)\n",
      "  (1): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# just to make sure that we successfully changed\n",
    "# number of output features \n",
    "print(model_3.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 1: 100%|██████████| 32/32 [01:50<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 1 Completed: Average loss: 4.602927\tAccuracy: 2.843% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [01:40<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 4.307562\tAccuracy: 15.59%\tF-1 Score: 0.1206\n",
      "The best model was saved with accuracy: 15.59%\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 2: 100%|██████████| 32/32 [01:48<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 2 Completed: Average loss: 4.205982\tAccuracy: 15.784% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [01:43<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.974060\tAccuracy: 33.33%\tF-1 Score: 0.2914\n",
      "The best model was saved with accuracy: 33.33%\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 3: 100%|██████████| 32/32 [01:55<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 3 Completed: Average loss: 3.844655\tAccuracy: 33.627% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [01:39<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.645155\tAccuracy: 47.06%\tF-1 Score: 0.4340\n",
      "The best model was saved with accuracy: 47.06%\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 4: 100%|██████████| 32/32 [01:48<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 4 Completed: Average loss: 3.469561\tAccuracy: 51.373% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [01:38<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.365293\tAccuracy: 52.45%\tF-1 Score: 0.4857\n",
      "The best model was saved with accuracy: 52.45%\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 5: 100%|██████████| 32/32 [01:48<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 5 Completed: Average loss: 3.164243\tAccuracy: 62.157% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [01:37<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.112697\tAccuracy: 58.82%\tF-1 Score: 0.5542\n",
      "The best model was saved with accuracy: 58.82%\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 6: 100%|██████████| 32/32 [01:48<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 6 Completed: Average loss: 2.904836\tAccuracy: 75.392% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [01:54<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.075557\tAccuracy: 61.08%\tF-1 Score: 0.5798\n",
      "The best model was saved with accuracy: 61.08%\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 7: 100%|██████████| 32/32 [01:54<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 7 Completed: Average loss: 2.863517\tAccuracy: 79.608% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [01:36<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.039001\tAccuracy: 64.22%\tF-1 Score: 0.6149\n",
      "The best model was saved with accuracy: 64.22%\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 8: 100%|██████████| 32/32 [01:47<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 8 Completed: Average loss: 2.825759\tAccuracy: 82.255% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [01:36<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 3.014183\tAccuracy: 64.12%\tF-1 Score: 0.6119\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 9: 100%|██████████| 32/32 [01:46<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 9 Completed: Average loss: 2.796082\tAccuracy: 83.039% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [01:36<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 2.988901\tAccuracy: 66.27%\tF-1 Score: 0.6351\n",
      "The best model was saved with accuracy: 66.27%\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 10: 100%|██████████| 32/32 [01:45<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 10 Completed: Average loss: 2.763920\tAccuracy: 83.725% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valodation: 100%|██████████| 32/32 [01:34<00:00,  2.94s/it]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_14776\\1588860085.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"Best_in_transfer_learning_CNN.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validation Completed: Average Loss: 2.976717\tAccuracy: 64.90%\tF-1 Score: 0.6196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 193/193 [30:44<00:00,  9.56s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Test Completed: Avverage loss: 3.104836\tAccuracy: 62.86%\tF-1 Score: 0.6136\n",
      "Test model was saved with Accuracy: 62.86% and Avverage Loss: 3.104836 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "writer_3 = SummaryWriter(log_dir=\"Transfer_learning_Model\")\n",
    "\n",
    "epochs = 10 \n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "patience = 3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model_3.to(device)\n",
    "optimizer_3 = optim.SGD(model.parameters(), lr = learning_rate, momentum=momentum)\n",
    "scheduler_3 = StepLR(optimizer_3, step_size=5,gamma=0.1)\n",
    "\n",
    "best_accuracy_3 = 0\n",
    "early_stop_counter_3 = 0 \n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "    train_1(model, device, train_loader, train_dataset, optimizer_3, epoch, writer_3)\n",
    "\n",
    "    val_loss_3, val_acc_3 = validation_1(model,device, val_loader, val_dataset, epoch, writer_3)\n",
    "\n",
    "    if val_acc_3 > best_accuracy_3:\n",
    "        best_accuracy_3 = val_acc_3\n",
    "        torch.save(model.state_dict(), \"Best_in_transfer_learning_CNN.pt\")\n",
    "        print(f\"The best model was saved with accuracy: {best_accuracy_3:.2f}%\")\n",
    "        early_stop_counter_3 = 0 \n",
    "    else:\n",
    "        early_stop_counter_3 +=1 \n",
    "\n",
    "    if early_stop_counter_3 >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch} due to no improvement in test accuracy.\")\n",
    "        break\n",
    "\n",
    "    scheduler_3.step()\n",
    "\n",
    "model.load_state_dict(torch.load(\"Best_in_transfer_learning_CNN.pt\"))\n",
    "\n",
    "test_loss_3, test_acc_3 = test_1(model,device, test_loader, test_dataset)\n",
    "torch.save(model.state_dict(), \"Test_result_for_transfer_learning.pt\")\n",
    "\n",
    "print(f\"Test model was saved with Accuracy: {test_acc_3:.2f}% and Avverage Loss: {test_loss_3:.6f} \")\n",
    "\n",
    "writer_3.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "We can see that thanks to pretrained `resnet50` model we got `Accuracy = 62.86%` on test set. As u remember well in the begining on baseline model we got less then `20%` of accuracy. \n",
    "Why it happend u may ask. Cause we've used pretrained model. That was trained on a vary large dataset and used same architecture with same weights on our data. We just changed final Fully Connected Layer, cause in our task we have 102 classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
